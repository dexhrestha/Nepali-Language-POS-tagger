{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagger model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from JSON"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 2,
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join('json/gc/webtext/json',x) for x in os.listdir('json/gc/webtext/json')]\n",
    "corpus = []\n",
    "\n",
    "for x in files:\n",
    "    with open(x,'r',encoding='utf-16') as f:\n",
    "#         print('opening {0}'.format(x))\n",
    "        doc = json.load(f)\n",
    "        for i in doc.keys():\n",
    "#             print(\"key {0} of document {1}\".format(i,x))\n",
    "            try:\n",
    "#                 print(doc[i][0][0])\n",
    "                if (len(doc[i][0][0])<200  and len(i) >0): #150 190 gave same max len\n",
    "                       corpus.append(doc[i][0][0])  #append each sentence to corpus\n",
    "            except:\n",
    "# #                 print(doc[i])\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 3,
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "\n",
    "files = [os.path.join('json/gc/books',x) for x in os.listdir('json/gc/books')]\n",
    "for x in files:\n",
    "    with open(x,'r',) as f:\n",
    "#         print('opening {0}'.format(x))\n",
    "        doc = json.load(f)\n",
    "#         for i in doc.keys():\n",
    "        doc = doc ['ps']\n",
    "        for i in doc:\n",
    "            if len(i) <200 and len(i) >0:\n",
    "                corpus.append(i)\n",
    "#                 print(i)\n",
    "            corpus = []\n",
    "\n",
    "%reset_selective -f doc\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence:  ['तिब्बती', 'किसान', 'तान', 'चंग', 'को', 'सम्पन्नता', 'को', 'कथा', 'को', 'दोस्रो', 'भाग']\n",
      "Sample sentence tags:  ['JX', 'NN', 'NN', 'NN', 'IKM', 'NN', 'IKM', 'NN', 'IKM', 'MOM', 'NN']\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "file_name = \"../Dataset/dataset_cs.csv\""
=======
    "sentences = []\n",
    "sentence_tags = []\n",
    "\n",
    "for sentence in corpus:\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for word in sentence:\n",
    "        x.append(word[1])\n",
    "        y.append(word[0])\n",
    "    if len(x) > 0:\n",
    "        sentences.append(x)\n",
    "        sentence_tags.append(y)\n",
    "# sentences = sentences[:8000]\n",
    "# sentence_tags = sentence_tags[:8000]\n",
    "print(\"Sample sentence: \",sentences[10])\n",
    "print(\"Sample sentence tags: \",sentence_tags[10])"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged_sentences 8000\n"
     ]
    }
   ],
   "source": [
    "print(\"Tagged_sentences\",len(sentence_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence:  ['कस्मिक', 'को', 'तेस्रो', 'विमान', 'फोकर', '–', 100]\n",
      "Sample sentence tags:  ['JX', 'IKM', 'MOM', 'NN', 'NN', 'YM', 'MM']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample sentence: \",sentences[112])\n",
    "print(\"Sample sentence tags: \",sentence_tags[112])"
   ]
  },
  {
   "cell_type": "markdown",
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "import pandas as pd\n",
    "df = pd.read_csv(file_name)\n",
    "df = df[[\"tags\",\"words\"]]\n",
    "#convert csv to list\n",
    "corpus = []\n",
    "sentence = []\n",
    "for tag,word in zip(df[\"tags\"],df[\"words\"]):\n",
    "    if(tag!='.'):\n",
    "        sentence.append((tag,word))\n",
    "    else:\n",
    "        corpus.append(sentence)\n",
    "        sentence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(corpus))"
=======
    "## Convert Labels to numbers"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "sentence_tags = []\n",
    "\n",
    "for sentence in corpus:\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for word in sentence:\n",
    "        x.append(word[1])\n",
    "        y.append(word[0])\n",
    "    if len(x) > 0:\n",
    "        sentences.append(x)\n",
    "        sentence_tags.append(y)\n"
=======
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tags:  96\n"
     ]
    }
   ],
   "source": [
    "labels = set()\n",
    "for sentence in sentence_tags:\n",
    "    for tag in sentence:\n",
    "        labels.add(tag)\n",
    "        \n",
    "tag2index = {t: i + 1 for i, t in enumerate(list(labels))}\n",
    "tag2index\n",
    "print(\"Total number of tags: \",len(labels))"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Tagged_sentences\",len(sentence_tags))"
=======
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load tag2index\n",
    "\n",
    "with open('withMaskingtag2index.pickle','rb') as f:\n",
    "    tag2index = pickle.load(f)"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample sentence: \",sentences[112])\n",
    "print(\"Sample sentence tags: \",sentence_tags[112])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Labels to numbers"
=======
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tag2index['-PAD-'] = 0\n"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "labels = []\n",
    "\n",
    "# labels = set()\n",
    "for sentence in sentence_tags:\n",
    "    labels.extend(sentence)\n",
    "\n",
    "labels = Counter(labels)\n",
    "tag2index = {t: i + 1 for i, t in enumerate(list(labels))}\n",
    "tag2index\n",
    "print(\"Total number of tags: \",len(labels))\n"
=======
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence:  ['तिब्बती', 'किसान', 'तान', 'चंग', 'को', 'सम्पन्नता', 'को', 'कथा', 'को', 'दोस्रो', 'भाग']\n",
      "Sample sentence tags:  [87, 18, 18, 18, 57, 18, 57, 18, 57, 95, 18]\n"
     ]
    }
   ],
   "source": [
    "def tagsent2int(sent_tag):\n",
    "    return [tag2index[tag] for tag in sent_tag]\n",
    "\n",
    "sentence_tags = list(map(tagsent2int,sentence_tags))\n",
    "print(\"Sample sentence: \",sentences[10])\n",
    "print(\"Sample sentence tags: \",sentence_tags[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load word2vec model"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load tag2index\n",
    "\n",
    "with open('withMaskingtag2index.pickle','rb') as f:\n",
    "    tag2index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tag2index['-PAD-'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tagsent2int(sent_tag):\n",
    "    return [tag2index[tag] for tag in sent_tag]\n",
    "\n",
    "sentence_tags = list(map(tagsent2int,sentence_tags))\n",
    "print(\"Sample sentence: \",sentences[10])\n",
    "print(\"Sample sentence tags: \",sentence_tags[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load word2vec model"
=======
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.word2vec as w2v\n",
    "nepw2v=w2v.Word2Vec.load('w2vmodel')"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.word2vec as w2v\n",
    "nepw2v=w2v.Word2Vec.load('../nepw2vmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tokenizer\n",
    "\n",
    "with open('withMaskingTokenizer.pickle','rb') as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=False, oov_token='-OOV-')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "encoded_docs = tokenizer.texts_to_sequences(sentences)\n",
    "max_length = len(max(sentences,key=len))\n",
    "# max_length = 197\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "padded_tags = pad_sequences(sentence_tags,maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length"
=======
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tokenizer\n",
    "\n",
    "with open('withMaskingTokenizer.pickle','rb') as f:\n",
    "    tokenizer = pickle.load(f)"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['-PAD-'] = 0"
=======
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(lower=False, oov_token='-OOV-')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "encoded_docs = tokenizer.texts_to_sequences(sentences)\n",
    "# max_length = len(max(sentences,key=len))\n",
    "max_length = 197\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "padded_tags = pad_sequences(sentence_tags,maxlen=max_length, padding='post')"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "nepw2v.wv.vector_size"
=======
    "tokenizer.word_index['-PAD-'] = 0"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('withMaskingtag2index.pickle','wb') as f:\n",
    "#     pickle.dump(tag2index,f)\n",
    "# with open('nepw2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
<<<<<<< HEAD
    "embedding_matrix = np.zeros((vocab_size, nepw2v.wv.vector_size))\n",
=======
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
    "for word, i in tokenizer.word_index.items():\n",
    "#     print(word)\n",
    "    try:\n",
    "        embedding_vector = nepw2v.wv.get_vector(word)\n",
    "    except:\n",
    "        embedding_vector = None\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('nepw2vembeddingmatrix.pickle','wb') as f:\n",
    "#     pickle.dump(embedding_matrix,f)\n",
    "# with open('withMaskingTokenizer.pickle','wb') as f:\n",
    "#     pickle.dump(tokenizer,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Generator"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  data_generator(sentences,sentence_tags,batch_size=3200):\n",
    "    num_samples = len(sentences)\n",
    "    print(num_samples)\n",
    "    while(True):\n",
    "        for offset in range(0,num_samples,batch_size):\n",
    "            batch_sentences = sentences[offset:offset+batch_size]\n",
    "            batch_sentence_tags = sentence_tags[offset:offset+batch_size]\n",
    "            batch_sentence_tags=to_categorical(batch_sentence_tags,num_classes=len(labels))\n",
    "            yield(batch_sentences,batch_sentence_tags)"
=======
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# skf = StratifiedKFold(n_splits=3,shuffle=True)\n",
    "train_sentences,test_sentences,train_sentence_tags,test_sentence_tags = train_test_split(padded_docs,padded_tags,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dexhrestha/anaconda3/envs/tf-cpu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/dexhrestha/anaconda3/envs/tf-cpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 197, 300)          5382300   \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, 197, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 197, 256)          570368    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 197, 97)           24929     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 197, 97)           0         \n",
      "=================================================================\n",
      "Total params: 5,977,597\n",
      "Trainable params: 595,297\n",
      "Non-trainable params: 5,382,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Dropout, Bidirectional, TimeDistributed, Embedding, Activation,Masking\n",
    "from keras.optimizers import Adam\n",
    " \n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(max_length,)))\n",
    "model.add(Embedding(vocab_size,300,weights=[embedding_matrix],input_length=max_length,trainable=False))\n",
    "model.add(Masking())\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5120 samples, validate on 1280 samples\n",
      "Epoch 1/10\n",
      "5120/5120 [==============================] - 50s 10ms/step - loss: 3.8914 - acc: 0.3686 - val_loss: 1.2901 - val_acc: 0.6992\n",
      "Epoch 2/10\n",
      "5120/5120 [==============================] - 51s 10ms/step - loss: 1.0408 - acc: 0.7569 - val_loss: 0.7880 - val_acc: 0.8079\n",
      "Epoch 3/10\n",
      "5120/5120 [==============================] - 51s 10ms/step - loss: 0.7312 - acc: 0.8285 - val_loss: 0.6282 - val_acc: 0.8524\n",
      "Epoch 4/10\n",
      "5120/5120 [==============================] - 52s 10ms/step - loss: 0.6034 - acc: 0.8579 - val_loss: 0.5460 - val_acc: 0.8702\n",
      "Epoch 5/10\n",
      "5120/5120 [==============================] - 51s 10ms/step - loss: 0.5281 - acc: 0.8745 - val_loss: 0.4923 - val_acc: 0.8825\n",
      "Epoch 6/10\n",
      "5120/5120 [==============================] - 51s 10ms/step - loss: 0.4747 - acc: 0.8862 - val_loss: 0.4542 - val_acc: 0.8898\n",
      "Epoch 7/10\n",
      "5120/5120 [==============================] - 52s 10ms/step - loss: 0.4340 - acc: 0.8949 - val_loss: 0.4265 - val_acc: 0.8958\n",
      "Epoch 8/10\n",
      "5120/5120 [==============================] - 52s 10ms/step - loss: 0.4049 - acc: 0.9014 - val_loss: 0.4030 - val_acc: 0.8994\n",
      "Epoch 9/10\n",
      "5120/5120 [==============================] - 53s 10ms/step - loss: 0.3789 - acc: 0.9056 - val_loss: 0.3849 - val_acc: 0.9027\n",
      "Epoch 10/10\n",
      "5120/5120 [==============================] - 51s 10ms/step - loss: 0.3585 - acc: 0.9107 - val_loss: 0.3696 - val_acc: 0.9062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f39dc6afa90>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_sentences,to_categorical(train_sentence_tags),batch_size=256,epochs=10,validation_split=0.2)"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Train Test split"
=======
    "## Save Model"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# skf = StratifiedKFold(n_splits=3,shuffle=True)\n",
    "train_sentences,test_sentences,train_sentence_tags,test_sentence_tags = train_test_split(padded_docs,padded_tags,test_size=0.2)"
=======
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import save_model\n",
    "save_model(model,'withMasking.h5')"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Train"
=======
    "## Load Model"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Dropout, Bidirectional, TimeDistributed, Embedding, Activation,Masking\n",
    "from keras.optimizers import Adam\n",
    " \n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(max_length,)))\n",
    "model.add(Embedding(vocab_size,150,weights=[embedding_matrix],input_length=max_length,trainable=False))\n",
    "model.add(Masking())\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(len(tag2index)-1)))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.fit(train_sentences,to_categorical(train_sentence_tags),batch_size=512,epochs=5,validation_split=0.2)\n"
=======
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dexhrestha/anaconda3/envs/tf-cpu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/dexhrestha/anaconda3/envs/tf-cpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/dexhrestha/anaconda3/envs/tf-cpu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/dexhrestha/anaconda3/envs/tf-cpu/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('withMasking.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Test model"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 197)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "import math\n",
    "math.ceil(3000/512)"
=======
    "test_sentence_tags.shape"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.0000576e-02, 9.7415270e-03, 1.0604734e-02, ...,\n",
       "         1.0260506e-02, 1.0235737e-02, 9.9062566e-03],\n",
       "        [1.2381864e-03, 2.7797890e-03, 9.5776487e-03, ...,\n",
       "         5.6460691e-03, 2.1177186e-03, 1.8547591e-03],\n",
       "        [1.2381864e-03, 2.7797890e-03, 9.5776487e-03, ...,\n",
       "         5.6460691e-03, 2.1177186e-03, 1.8547591e-03],\n",
       "        ...,\n",
       "        [1.9855240e-06, 3.1439006e-06, 5.4778258e-05, ...,\n",
       "         8.3695231e-06, 3.6361980e-06, 8.6164812e-07],\n",
       "        [1.9855240e-06, 3.1439006e-06, 5.4778258e-05, ...,\n",
       "         8.3695231e-06, 3.6361980e-06, 8.6164812e-07],\n",
       "        [1.9855240e-06, 3.1439006e-06, 5.4778258e-05, ...,\n",
       "         8.3695231e-06, 3.6361980e-06, 8.6164812e-07]]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "train_sentences,val_sentences,test_sentence_tags,val_sentence_tags = train_test_split(train_sentences,train_sentence_tags,test_size=0.2)"
=======
    "# model.evaluate(test_sentences,to_categorical(test_sentence_tags))\n",
    "# model.evalutate(sentences,to_categorical(sentence_tags))\n",
    "model.predict(np.asarray([padded_docs[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "#             print(np.argmax(categorical))\n",
    "\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    "\n",
    "#                 token_sequence.append(index[np.argmax(0)])\n",
    "        token_sequences.append(token_sequence)\n",
    " \n",
    "    return token_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_sentence(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "#             try:\n",
    "            token_sequence.append(index[categorical])\n",
    "\n",
    "        token_sequences.append(token_sequence)\n",
    "    return token_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('withMaskingTokenizer.pickle','wb') as f:\n",
    "#     pickle.dump(tokenizer,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.asarray([padded_docs[127]])\n",
    "test_tag = np.asarray([padded_tags[127]])\n"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "train_generator = data_generator(train_sentences,train_sentence_tags,3000)\n",
    "val_generator = data_generator(val_sentences,val_sentence_tags,1000)\n",
    "model.fit_generator(train_generator,samples_per_epoch=6,epochs=10,verbose=1,validation_data=val_generator,validation_steps=100)\n"
=======
    "predicted =model.predict(test)"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "## Save Model"
=======
    "# with open('withMaskingTokenizer.pickle','rb') as f:\n",
    "#     tokenizer = pickle.load(f)\n",
    "    \n",
    "\n",
    "# with open('withMaskingtag2index.pickle','rb') as f:\n",
    "#     tag2index = pickle.load(f)"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import save_model\n",
    "save_model(model,'withMasking.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
=======
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer.word_index['-00V-'] = 0\n",
    "p_sent_tag = logits_to_tokens(predicted, {i: t for t, i in tag2index.items()})\n",
    "a_sent_tag = logits_to_sentence(test_tag, {i: t for t, i in tag2index.items()})\n",
    "p_sent = logits_to_sentence(test, {i: t for t, i in tokenizer.word_index.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('मेरी', 'PMXKF')\n",
      "('छोरी', 'NN')\n",
      "('को', 'IKM')\n",
      "('देश', 'NN')\n"
     ]
    }
   ],
   "source": [
    "for word,tag in zip(p_sent,a_sent_tag):\n",
    "    if word!='-00V-' :\n",
    "        print((word,tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('मेरी', 'RR'), ('छोरी', 'NN'), ('को', 'IKM'), ('देश', 'DKX')]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for word,tag in zip(p_sent,p_sent_tag):\n",
    "    if word!='-00V-' :\n",
    "        predicted.append((word,tag))\n",
    "print(predicted)"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('withMasking.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_sentences,to_categorical(test_sentence_tags))\n",
    "# model.evalutate(sentences,to_categorical(sentence_tags))\n",
    "# model.predict(np.asarray([padded_docs[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "#             print(np.argmax(categorical))\n",
    "\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    "\n",
    "#                 token_sequence.append(index[np.argmax(0)])\n",
    "        token_sequences.append(token_sequence)\n",
    " \n",
    "    return token_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_sentence(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "#             try:\n",
    "            token_sequence.append(index[categorical])\n",
    "\n",
    "        token_sequences.append(token_sequence)\n",
    "    return token_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('withMaskingTokenizer.pickle','wb') as f:\n",
    "#     pickle.dump(tokenizer,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[:,:-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = np.asarray([padded_docs[-12]])\n",
    "test_tag = np.asarray([padded_tags[-12]])\n",
    "test= test[:,:-1]\n",
    "test_tag = test_tag[:,:-1]"
=======
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import save_model\n",
    "\n",
    "save_model(model,'withMasking.h5',include_optimizer=True)"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted =model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('withMaskingTokenizer.pickle','rb') as f:\n",
    "#     tokenizer = pickle.load(f)\n",
    "    \n",
    "\n",
    "# with open('withMaskingtag2index.pickle','rb') as f:\n",
    "#     tag2index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 83,
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "tokenizer.word_index['-00V-'] = 0\n",
    "p_sent_tag = logits_to_tokens(predicted, {i: t for t, i in tag2index.items()})\n",
    "a_sent_tag = logits_to_sentence(test_tag, {i: t for t, i in tag2index.items()})\n",
    "p_sent = logits_to_sentence(test, {i: t for t, i in tokenizer.word_index.items()})"
=======
    "from keras import backend as K\n",
    " \n",
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'float32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'float32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy\n",
    " "
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [ ]\n",
    "for word,tag in zip(p_sent,a_sent_tag):\n",
    "    if word!='-00V-' :\n",
    "        print((word,tag))\n",
    "#         t.append(word)\n",
    "#         print(t)"
=======
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 197, 300)          5382300   \n",
      "_________________________________________________________________\n",
      "masking_4 (Masking)          (None, 197, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 197, 256)          570368    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 197, 97)           24929     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 197, 97)           0         \n",
      "=================================================================\n",
      "Total params: 5,977,597\n",
      "Trainable params: 595,297\n",
      "Non-trainable params: 5,382,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(max_length,)))\n",
    "model.add(Embedding(vocab_size,300,weights=[embedding_matrix],input_length=max_length,trainable=False))\n",
    "model.add(Masking())\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
    " \n",
    "model.summary()\n",
    " "
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5120 samples, validate on 1280 samples\n",
      "Epoch 1/5\n",
      "5120/5120 [==============================] - 59s 11ms/step - loss: 2.2438 - acc: 0.5538 - ignore_accuracy: 0.0351 - val_loss: 1.0832 - val_acc: 0.7650 - val_ignore_accuracy: 0.0481\n",
      "Epoch 2/5\n",
      "5120/5120 [==============================] - 59s 12ms/step - loss: 0.8726 - acc: 0.8044 - ignore_accuracy: 0.0496 - val_loss: 0.6756 - val_acc: 0.8478 - val_ignore_accuracy: 0.0529\n",
      "Epoch 3/5\n",
      "5120/5120 [==============================] - 62s 12ms/step - loss: 0.6040 - acc: 0.8625 - ignore_accuracy: 0.0529 - val_loss: 0.5271 - val_acc: 0.8780 - val_ignore_accuracy: 0.0547\n",
      "Epoch 4/5\n",
      "5120/5120 [==============================] - 60s 12ms/step - loss: 0.4818 - acc: 0.8878 - ignore_accuracy: 0.0543 - val_loss: 0.4494 - val_acc: 0.8919 - val_ignore_accuracy: 0.0554\n",
      "Epoch 5/5\n",
      "5120/5120 [==============================] - 59s 11ms/step - loss: 0.4126 - acc: 0.8996 - ignore_accuracy: 0.0550 - val_loss: 0.4053 - val_acc: 0.9000 - val_ignore_accuracy: 0.0559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3a31ad06d8>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_sentences, to_categorical(train_sentence_tags, len(tag2index)), batch_size=128, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "predicted = []\n",
    "for word,tag in zip(p_sent,p_sent_tag):\n",
    "    if word!='-00V-' :\n",
    "        predicted.append((word,tag))\n",
    "print(predicted)"
=======
    "from keras.models import load_model\n",
    "model = load_model('withMasking_ignore_accuracy.h5')"
>>>>>>> e997dd64eb8c0825dbf37eb4a2961649d80274e2
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import save_model\n",
    "\n",
    "save_model(model,'withMasking.h5',include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
