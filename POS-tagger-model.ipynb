{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagger model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join('json/gc/webtext/json',x) for x in os.listdir('json/gc/webtext/json')]\n",
    "corpus = []\n",
    "\n",
    "for x in files:\n",
    "    with open(x,'r',encoding='utf-16') as f:\n",
    "#         print('opening {0}'.format(x))\n",
    "        doc = json.load(f)\n",
    "        for i in doc.keys():\n",
    "#             print(\"key {0} of document {1}\".format(i,x))\n",
    "            try:\n",
    "#                 print(doc[i][0][0])\n",
    "                if (len(doc[i][0][0])<200  and len(i) >0): #150 190 gave same max len\n",
    "                       corpus.append(doc[i][0][0])  #append each sentence to corpus\n",
    "            except:\n",
    "# #                 print(doc[i])\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "files = [os.path.join('json/gc/books',x) for x in os.listdir('json/gc/books')]\n",
    "for x in files:\n",
    "    with open(x,'r',) as f:\n",
    "#         print('opening {0}'.format(x))\n",
    "        doc = json.load(f)\n",
    "#         for i in doc.keys():\n",
    "        doc = doc ['ps']\n",
    "        for i in doc:\n",
    "            if len(i) <200 and len(i) >0:\n",
    "                corpus.append(i)\n",
    "#                 print(i)\n",
    "            \n",
    "%reset_selective -f doc\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = '../major8sem/Dataset/dataset_cs.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_url)\n",
    "df = df[[\"tags\",\"words\"]]\n",
    "corpus = []\n",
    "sentence = []\n",
    "for tag,word in zip(df[\"tags\"],df[\"words\"]):\n",
    "    if(tag!='.'):\n",
    "        sentence.append((tag,word))\n",
    "    else:\n",
    "        corpus.append(sentence)\n",
    "        sentence = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59668\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence:  ['उत्पादन', ',', 'वित्तीय', 'स्थिति', ',', 'वातावरण', 'को', 'कुरो', ',', 'बजार', 'उपलब्धता', 'आदि', 'ले', 'ऊनी', 'गलैंचा', 'को', 'निकासी', 'प्रभावित', 'पार्ने', 'बताउँदै', 'अध्यक्ष', 'श्री', 'श्रेष्ठ', 'ले', 'गलैंचा', 'धुवाई', 'मेशिन', 'हरू', 'ल्याएर', 'गलैंचा', 'हरू', 'को', 'धुवाई', 'नेपालमै', 'गरिना', 'ले', 'गलैंचा', 'व्यवसाय', 'सँग', 'सम्बन्धित', 'विदेशी', 'उद्यमी', 'हरू', 'लाई', 'प्रतिकूल', 'असर', 'पारे', 'को', 'ले', 'ती', 'उद्यमी', 'हरू', 'ले', 'नेपाल', 'मा', 'धुवाइ', 'गरिए', 'का', 'गलैंचा', 'हरू', 'प्रदूषित', 'हुने', 'हल्ला', 'गर्न', 'थाले', 'का', 'छन्', '।']\n",
      "Sample sentence tags:  ['NN', 'YM', 'JX', 'NN', 'YM', 'NN', 'IKM', 'NN', 'YM', 'NN', 'NN', 'RR', 'IE', 'DDX', 'NN', 'IKM', 'NN', 'JX', 'VN', 'VDX', 'NN', 'NN', 'NP', 'IE', 'NN', 'NN', 'NN', 'IH', 'VQ', 'NN', 'IH', 'IKM', 'NN', 'NN', 'VI', 'IE', 'NN', 'NN', 'II', 'JX', 'JX', 'NN', 'IH', 'IA', 'JX', 'NN', 'VE', 'IKM', 'IE', 'DDX', 'NN', 'IH', 'IE', 'NP', 'II', 'NN', 'VE', 'IKO', 'NN', 'IH', 'JX', 'VN', 'NN', 'VI', 'VE', 'IKO', 'VVYX2', 'YF']\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "sentence_tags = []\n",
    "\n",
    "for sentence in corpus:\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for word in sentence:\n",
    "        x.append(word[1])\n",
    "        y.append(word[0])\n",
    "    if len(x) > 0:\n",
    "        sentences.append(x)\n",
    "        sentence_tags.append(y)\n",
    "# sentences = sentences[:8000]\n",
    "# sentence_tags = sentence_tags[:8000]\n",
    "print(\"Sample sentence: \",sentences[10])\n",
    "print(\"Sample sentence tags: \",sentence_tags[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged_sentences 59375\n"
     ]
    }
   ],
   "source": [
    "print(\"Tagged_sentences\",len(sentence_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence:  ['हाइटी', 'का', 'अपदस्थ', 'राष्ट्रपति', 'आरिष्टिड', 'सँग', 'अमेरिकी', 'राष्ट्रपति', 'बुश', 'को', 'यो', 'पहिलो', 'भेट', 'हो', '।']\n",
      "Sample sentence tags:  ['NP', 'IKO', 'JX', 'NN', 'NP', 'II', 'JX', 'NN', 'NP', 'IKM', 'DDX', 'MOM', 'NN', 'VVYN1', 'YF']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample sentence: \",sentences[112])\n",
    "print(\"Sample sentence tags: \",sentence_tags[112])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Labels to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tags:  116\n"
     ]
    }
   ],
   "source": [
    "labels = set()\n",
    "for sentence in sentence_tags:\n",
    "    for tag in sentence:\n",
    "        labels.add(tag)\n",
    "        \n",
    "tag2index = {t: i + 1 for i, t in enumerate(list(labels))}\n",
    "tag2index\n",
    "print(\"Total number of tags: \",len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Load tag2index\n",
    "\n",
    "# with open('withMaskingtag2index.pickle','rb') as f:\n",
    "#     tag2index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tag2index['-PAD-'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence:  ['उत्पादन', ',', 'वित्तीय', 'स्थिति', ',', 'वातावरण', 'को', 'कुरो', ',', 'बजार', 'उपलब्धता', 'आदि', 'ले', 'ऊनी', 'गलैंचा', 'को', 'निकासी', 'प्रभावित', 'पार्ने', 'बताउँदै', 'अध्यक्ष', 'श्री', 'श्रेष्ठ', 'ले', 'गलैंचा', 'धुवाई', 'मेशिन', 'हरू', 'ल्याएर', 'गलैंचा', 'हरू', 'को', 'धुवाई', 'नेपालमै', 'गरिना', 'ले', 'गलैंचा', 'व्यवसाय', 'सँग', 'सम्बन्धित', 'विदेशी', 'उद्यमी', 'हरू', 'लाई', 'प्रतिकूल', 'असर', 'पारे', 'को', 'ले', 'ती', 'उद्यमी', 'हरू', 'ले', 'नेपाल', 'मा', 'धुवाइ', 'गरिए', 'का', 'गलैंचा', 'हरू', 'प्रदूषित', 'हुने', 'हल्ला', 'गर्न', 'थाले', 'का', 'छन्', '।']\n",
      "Sample sentence tags:  [61, 56, 70, 61, 56, 61, 48, 61, 56, 61, 61, 82, 67, 39, 61, 48, 61, 70, 52, 86, 61, 61, 37, 67, 61, 61, 61, 81, 95, 61, 81, 48, 61, 61, 116, 67, 61, 61, 30, 70, 70, 61, 81, 114, 70, 61, 101, 48, 67, 39, 61, 81, 67, 37, 30, 61, 101, 10, 61, 81, 70, 52, 61, 116, 101, 10, 14, 4]\n"
     ]
    }
   ],
   "source": [
    "def tagsent2int(sent_tag):\n",
    "    return [tag2index[tag] for tag in sent_tag]\n",
    "\n",
    "sentence_tags = list(map(tagsent2int,sentence_tags))\n",
    "print(\"Sample sentence: \",sentences[10])\n",
    "print(\"Sample sentence tags: \",sentence_tags[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.word2vec as w2v\n",
    "nepw2v=w2v.Word2Vec.load('../major8sem/nepw2vmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tokenizer\n",
    "\n",
    "# with open('withMaskingTokenizer.pickle','rb') as f:\n",
    "#     tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=False, oov_token='-OOV-')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "encoded_docs = tokenizer.texts_to_sequences(sentences)\n",
    "# max_length = len(max(sentences,key=len))\n",
    "max_length = 197\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "padded_tags = pad_sequences(sentence_tags,maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['-PAD-'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('withMaskingtag2index.pickle','wb') as f:\n",
    "#     pickle.dump(tag2index,f)\n",
    "# with open('nepw2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, nepw2v.wv.vector_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "#     print(word)\n",
    "    try:\n",
    "        embedding_vector = nepw2v.wv.get_vector(word)\n",
    "    except:\n",
    "        embedding_vector = None\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('nepw2vembeddingmatrix.pickle','wb') as f:\n",
    "#     pickle.dump(embedding_matrix,f)\n",
    "# with open('withMaskingTokenizer.pickle','wb') as f:\n",
    "#     pickle.dump(tokenizer,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# skf = StratifiedKFold(n_splits=3,shuffle=True)\n",
    "train_sentences,test_sentences,train_sentence_tags,test_sentence_tags = train_test_split(padded_docs,padded_tags,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Dropout, Bidirectional, TimeDistributed, Embedding, Activation,Masking\n",
    "from keras.optimizers import Adam\n",
    " \n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(max_length,)))\n",
    "model.add(Embedding(vocab_size,300,weights=[embedding_matrix],input_length=max_length,trainable=False))\n",
    "model.add(Masking())\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True,drop)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_sentences,to_categorical(train_sentence_tags),batch_size=256,epochs=10,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import save_model\n",
    "save_model(model,'withMasking.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('withMasking.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_tags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.evaluate(test_sentences,to_categorical(test_sentence_tags))\n",
    "# model.evalutate(sentences,to_categorical(sentence_tags))\n",
    "model.predict(np.asarray([padded_docs[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_sentence(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "#             try:\n",
    "            token_sequence.append(index[categorical])\n",
    "\n",
    "        token_sequences.append(token_sequence)\n",
    "    return token_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('withMaskingTokenizer.pickle','wb') as f:\n",
    "#     pickle.dump(tokenizer,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.asarray([padded_docs[127]])\n",
    "test_tag = np.asarray([padded_tags[127]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted =model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('withMaskingTokenizer.pickle','rb') as f:\n",
    "#     tokenizer = pickle.load(f)\n",
    "    \n",
    "\n",
    "# with open('withMaskingtag2index.pickle','rb') as f:\n",
    "#     tag2index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer.word_index['-00V-'] = 0\n",
    "p_sent_tag = logits_to_tokens(predicted, {i: t for t, i in tag2index.items()})\n",
    "a_sent_tag = logits_to_sentence(test_tag, {i: t for t, i in tag2index.items()})\n",
    "p_sent = logits_to_sentence(test, {i: t for t, i in tokenizer.word_index.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word,tag in zip(p_sent,a_sent_tag):\n",
    "    if word!='-00V-' :\n",
    "        print((word,tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "for word,tag in zip(p_sent,p_sent_tag):\n",
    "    if word!='-00V-' :\n",
    "        predicted.append((word,tag))\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import save_model\n",
    "\n",
    "save_model(model,'withMasking.h5',include_optimizer=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
